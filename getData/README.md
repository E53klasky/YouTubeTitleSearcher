In order to get the data we need for the project, we must do the following steps to get the data in a useable format. The data we are collect will be in the form of video id, video title, view count, like count, and comment count (should we also get the date, guys? idk if that'll be important). We will be using the YouTube 8M dataset for this project. 

1. The YouTube 8m dataset is in the .tfrecord format, for machine learning purposes. To download the files, go to the YouTube 8M website (https://research.google.com/youtube8m/download.html). You will need to install python to download the files. We will be using the training dataset because it is the largest dataset (maybe we should use the other datasets too? idk).

2. Unfortunately, these .tfrecord files do not have any of the data we need for this project except for an encoded version of the video id that's only 4 digits long instead of 11. However, we can extract these video ids using the python code "datagetter.py". In order to use this file, you'll need to download the tensorflow packet for python: pip install tensorflow. If your version of python is too new (version 3.11+) downloading tensorflow will not work, so you may have to downgrade. The "datagetter.py" file will output all the encoded video ids into a file called "video_ids.txt". The process of extracting the encoded video ids is reletively fast.

3. Also unfortunately, these encoded video ids are useless on there own and must be converted to actual YouTube 11-digit video ids. To do this we must get it from a specified URL. More information about the decoding process can be found here: https://research.google.com/youtube8m/video_id_conversion.html. However, the gist of it is that you need to run this python code "decoder.py", which will output the actual video ids in "decoded_ids.txt". This operation takes a really, really long time (about 400,000 ids every 12 hours of runtime). Therefore, I will split up the encoded ids into sections of about 12 hours of runtime (depending on the speed of your Wi-Fi) that way we can all contribute a little bit to getting the data at a time every day. Just keep your computer on and connected to Wi-Fi over night to run the code. It can also run in the background with no problem.

4. After we get the video ids, we can finally access all the data we need regarding the video! We can use the YouTube API to get the data we need. For more info on the YouTube API: https://developers.google.com/youtube/v3 (also ask chatgpt for help). All you need to run though is the "apicaller.py" code. This will take the video ids and output the information we need into a text file called "data.txt". This operation will also take a very long time, so I will split up the ids into parts so we can do the same thing we did for the last step.

5. Now we are done! All of our data should be in a data.txt file in a comma-separated list with the information we need for this project. Oh, I almost forgot. Make sure to use unicode strings when accessing the titles of the videos because sometimes they have non-roman ascii characters (i.e. russian/greek letters, chinese symbols).
